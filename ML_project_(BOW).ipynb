{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML project (BOW).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "H9tMg5v7bVSO"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarjerw/TDT4173-project-group6/blob/main/ML_project_(BOW).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-InharmhPF7W"
      },
      "source": [
        "Political party analysis program that parses the tweets fetched from Twitter using Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDX2HpAbVCwU"
      },
      "source": [
        "###### IMPORT THE LIBRARIES  #########\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#import needed libraries\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZom6A0JPscH"
      },
      "source": [
        "##Retrieve data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvhRzsKoGpGc"
      },
      "source": [
        "#retrieves data\n",
        "tweets2016_df = pd.read_csv('TweetDatabase_2016.csv')\n",
        "tweets2020_df = pd.read_csv('TweetDatabase_2020.csv')\n",
        "df_2016 = tweets2016_df \n",
        "df_2020 = tweets2020_df \n",
        "\n",
        "#removes all duplicates\n",
        "df_2016 = df_2016.drop_duplicates()\n",
        "df_2020 = df_2020.drop_duplicates() \n",
        "df_2016_2020 = df_2016.append(df_2020)\n",
        "\n",
        "\n",
        "#shuffles data\n",
        "df_2016 = df_2016.sample(frac = 1)\n",
        "df_2020 = df_2020.sample(frac = 1)\n",
        "df_2016_2020 = df_2016_2020.sample(frac = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9tMg5v7bVSO"
      },
      "source": [
        "####Tweet count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmDRqfADfdIZ",
        "outputId": "1a1c5de3-1131-499b-f6cb-f829e39abe1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#counts total number of tweets per year\n",
        "print(df_2016.Tweet.count(),\"tweets in 2016\")\n",
        "print(df_2020.Tweet.count(),\"tweets in 2020\")\n",
        "print(df_2016_2020.Tweet.count(),\"tweets in 2016+2020 \\n\")\n",
        "\n",
        "\n",
        "#counts the number of democrate tweets and republican tweets\n",
        "print(\"Number of tweets year 2016:\")\n",
        "print(df_2016.Party.value_counts(), '\\n')\n",
        "print(\"Number of tweets year 2020:\")\n",
        "print(df_2020.Party.value_counts(), '\\n')\n",
        "print(\"Number of tweets year 2016+2020:\")\n",
        "print(df_2016_2020.Party.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6932 tweets in 2016\n",
            "5732 tweets in 2020\n",
            "12664 tweets in 2016+2020 \n",
            "\n",
            "Number of tweets year 2016:\n",
            "Democrat      3739\n",
            "Republican    3193\n",
            "Name: Party, dtype: int64 \n",
            "\n",
            "Number of tweets year 2020:\n",
            "Republican    3029\n",
            "Democrat      2703\n",
            "Name: Party, dtype: int64 \n",
            "\n",
            "Number of tweets year 2016+2020:\n",
            "Democrat      6442\n",
            "Republican    6222\n",
            "Name: Party, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yWOpVu5OJwB"
      },
      "source": [
        "##Sentiment Analysis using Bag-of-Words & N-Gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyygWCNVCal3"
      },
      "source": [
        "###Splitting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPAiM8lDhDKt",
        "outputId": "14e980a8-8157-48b3-e5f6-66d1a399d071",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Splits dataset into Training data and Test data, with 20% test data and 80% training data\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(df_2016.Tweet, df_2016.Party, test_size=0.2)\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(df_2020.Tweet, df_2020.Party, test_size=0.2)\n",
        "X_train1620, X_test1620, y_train1620, y_test1620 = train_test_split(df_2016_2020.Tweet, df_2016_2020.Party, test_size=0.2)\n",
        "\n",
        "\n",
        "print(\"Year 2016: \\n\"+\"Democratic republican ratio in Training data:\\n\"+str(y_train16.value_counts())+\"\\n\")\n",
        "print(\"Democratic republican ratio in Test data:\\n\"+str(y_test16.value_counts()),'\\n')\n",
        "print(\"-------------------------------------------------------\\n\")\n",
        "print(\"Year 2020: \\n\"+\"Democratic republican ratio in Training data:\\n\"+str(y_train20.value_counts())+\"\\n\")\n",
        "print(\"Democratic republican ratio in Test data:\\n\"+str(y_test20.value_counts()),'\\n')\n",
        "print(\"-------------------------------------------------------\\n\")\n",
        "print(\"Year 2016+2020: \\n\"+\"Democratic republican ratio in Training data:\\n\"+str(y_train1620.value_counts())+\"\\n\")\n",
        "print(\"Democratic republican ratio in Test data:\\n\"+str(y_test1620.value_counts()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Year 2016: \n",
            "Democratic republican ratio in Training data:\n",
            "Democrat      2997\n",
            "Republican    2548\n",
            "Name: Party, dtype: int64\n",
            "\n",
            "Democratic republican ratio in Test data:\n",
            "Democrat      742\n",
            "Republican    645\n",
            "Name: Party, dtype: int64 \n",
            "\n",
            "-------------------------------------------------------\n",
            "\n",
            "Year 2020: \n",
            "Democratic republican ratio in Training data:\n",
            "Republican    2402\n",
            "Democrat      2183\n",
            "Name: Party, dtype: int64\n",
            "\n",
            "Democratic republican ratio in Test data:\n",
            "Republican    627\n",
            "Democrat      520\n",
            "Name: Party, dtype: int64 \n",
            "\n",
            "-------------------------------------------------------\n",
            "\n",
            "Year 2016+2020: \n",
            "Democratic republican ratio in Training data:\n",
            "Democrat      5154\n",
            "Republican    4977\n",
            "Name: Party, dtype: int64\n",
            "\n",
            "Democratic republican ratio in Test data:\n",
            "Democrat      1288\n",
            "Republican    1245\n",
            "Name: Party, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bob8NcLkoB0C"
      },
      "source": [
        "####Creating Bag-of-Words Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SFhnV7boofN"
      },
      "source": [
        "# Create Bag of words for each election year and preprocess data\n",
        "# max_df removes all words that are too frequent in the dataset \n",
        "# min_df removes all words that occur only once in the dataset \n",
        "# stop_words='english' removes most common, irrelevant english words\n",
        "\n",
        "BagOfWords_vectorizer16 = CountVectorizer(lowercase=True, max_df=0.35, min_df=2, stop_words='english')\n",
        "BagOfWords_vectorizer20 = CountVectorizer(lowercase=True, max_df=0.35, min_df=2, stop_words='english')\n",
        "BagOfWords_vectorizer1620 = CountVectorizer(lowercase=True, max_df=0.35, min_df=2, stop_words='english')\n",
        "\n",
        "\n",
        "# fit(train_data) = learn the vocabulary of the training data\n",
        "# transform(train_data) = convert the training data into a document term matrix\n",
        "X_train_BagOfWords16 = BagOfWords_vectorizer16.fit_transform(X_train16) \n",
        "#transform(test_data) = use the fitted vocabulary (training) to build a document term matrix from the testing data\n",
        "X_test_BagOfWords16 = BagOfWords_vectorizer16.transform(X_test16)\n",
        "\n",
        "\n",
        "X_train_BagOfWords20 = BagOfWords_vectorizer20.fit_transform(X_train20)\n",
        "X_test_BagOfWords20 = BagOfWords_vectorizer20.transform(X_test20)\n",
        "\n",
        "X_train_BagOfWords1620 = BagOfWords_vectorizer1620.fit_transform(X_train1620)\n",
        "X_test_BagOfWords1620 = BagOfWords_vectorizer1620.transform(X_test1620)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCySJUzdDZat"
      },
      "source": [
        "####Creating N-Gram Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk8tS1kZDkUM"
      },
      "source": [
        "# Create N-Gram for each election year and preprocess data\n",
        "# max_df removes all words that are too frequent in the dataset \n",
        "# consider using TfidfVectorizer() instead of CountVectorizer()\n",
        "N_Gram_vectorizer16 = CountVectorizer(ngram_range=(1,4), max_df=0.35, min_df=2)\n",
        "X_train_N_Gram16 = N_Gram_vectorizer16.fit_transform(X_train16)\n",
        "X_test_N_Gram16 = N_Gram_vectorizer16.transform(X_test16)\n",
        "\n",
        "N_Gram_vectorizer20 = CountVectorizer(ngram_range=(1,4), max_df=0.35, min_df=2)\n",
        "X_train_N_Gram20 = N_Gram_vectorizer20.fit_transform(X_train20)\n",
        "X_test_N_Gram20 = N_Gram_vectorizer20.transform(X_test20)\n",
        "\n",
        "N_Gram_vectorizer1620 = CountVectorizer(ngram_range=(1,4), max_df=0.35, min_df=2)\n",
        "X_train_N_Gram1620 = N_Gram_vectorizer1620.fit_transform(X_train1620)\n",
        "X_test_N_Gram1620 = N_Gram_vectorizer1620.transform(X_test1620)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex0MwkPk2Psp"
      },
      "source": [
        "###Performing cross validation\n",
        "\n",
        "A key challenge with machine learning, is that we can’t know how well our model will perform on new data until we actually test it.\n",
        "\n",
        "To address this, we can split our initial dataset into separate training and test subsets.\n",
        "\n",
        "There are different types of Cross Validation Techniques but the overall concept remains the same:\n",
        "\n",
        "*   To partition the data into a number of subsets (given as argument for cv in cross_val_score below)\n",
        "*   Hold out a set at a time and train the model on remaining set\n",
        "*   Test model on hold out set\n",
        "*   Repeat the process for each subset of the dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A cross validation is done in the code segment below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt-a2v4Bk3c2",
        "outputId": "ab1ec89a-b0de-4df2-fced-37b124dee5b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross-validation is a statistical method used to estimate the skill of machine learning models.\n",
        "scores_BagOfWords16 = cross_val_score(LogisticRegression(), X_train_BagOfWords16, y_train16, cv=5)\n",
        "scores_BagOfWords20 = cross_val_score(LogisticRegression(), X_train_BagOfWords20, y_train20, cv=5)\n",
        "scores_BagOfWords1620 = cross_val_score(LogisticRegression(), X_train_BagOfWords1620, y_train1620, cv=5)\n",
        "\n",
        "scores_N_Gram16 = cross_val_score(LogisticRegression(), X_train_N_Gram16, y_train16, cv=5)\n",
        "scores_N_Gram20 = cross_val_score(LogisticRegression(), X_train_N_Gram20, y_train20, cv=5)\n",
        "scores_N_Gram1620 = cross_val_score(LogisticRegression(), X_train_N_Gram1620, y_train1620, cv=5)\n",
        "\n",
        "\n",
        "\n",
        "# Returns a table illustrating the cross validation scores of the training sets for both methods in 2016 and 2020.\n",
        "import TableIt\n",
        "myList = [\n",
        "    [\"Year      | Method\", \"Cross Validation scores\", \"Mean accuracy\"],\n",
        "    [\"2016      | Bag-of-Words\",scores_BagOfWords16,sum(scores_BagOfWords16)/len(scores_BagOfWords16)],\n",
        "    [\"          | N-Gram\",scores_N_Gram16, sum(scores_N_Gram16)/len(scores_N_Gram16)],\n",
        "    [\"2020      | Bag-of-Words\",scores_BagOfWords20, sum(scores_BagOfWords20)/len(scores_BagOfWords20)],\n",
        "    [\"          | N-Gram\",scores_N_Gram20, sum(scores_N_Gram20)/len(scores_N_Gram20)],\n",
        "    [\"2016+2020 | Bag-of-Words\",scores_BagOfWords1620, sum(scores_BagOfWords1620)/len(scores_BagOfWords1620)],\n",
        "    [\"          | N-Gram\",scores_N_Gram1620 , sum(scores_N_Gram1620)/len(scores_N_Gram1620)]\n",
        "]\n",
        "\n",
        "TableIt.printTable(myList, useFieldNames=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------+\n",
            "| Year      | Method                                       | Cross Validation scores                                  |\n",
            "+----------------------------------------------------------+----------------------------------------------------------+\n",
            "| 2016      | Bag-of-Words                                 | [0.9251578  0.93327322 0.91974752 0.93146979 0.93327322] |\n",
            "|           | N-Gram                                       | [0.9386835  0.93056808 0.93597836 0.94138864 0.93327322] |\n",
            "| 2020      | Bag-of-Words                                 | [0.95528899 0.94547437 0.94220284 0.92693566 0.94438386] |\n",
            "|           | N-Gram                                       | [0.94874591 0.94656489 0.9476554  0.94220284 0.9389313 ] |\n",
            "| 2016+2020 | Bag-of-Words                                 | [0.92846571 0.92941757 0.92793682 0.92102665 0.9254689 ] |\n",
            "|           | N-Gram                                       | [0.93685249 0.93435341 0.93830207 0.93336624 0.93484699] |\n",
            "+---------------------------------------------------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDIaIGzRCGSX"
      },
      "source": [
        "###Data Prediction\n",
        "\n",
        "How well does the training data predict the test data?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-zxaKFaKWUQ"
      },
      "source": [
        "Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64yVS3enKUiT"
      },
      "source": [
        "# How well does the Bag of Words training data predict the test data\n",
        "acc_table_train16 = []\n",
        "acc_table_test16 = []\n",
        "acc_table_train20 = []\n",
        "acc_table_test20 = []\n",
        "acc_table_train1620 = []\n",
        "acc_table_test1620 = []\n",
        "param_range= [0.001,0.01,0.05,0.1,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0,2.5,3,3.5,4,5,6,7,8,9,10]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, sharey='row', figsize=(22.5,8))\n",
        "\n",
        "for i in param_range:\n",
        "  #Apply logistic regression model to training data\n",
        "  logit_BoW_16 = LogisticRegression(C=i)\n",
        "  logit_BoW_20 = LogisticRegression(C=i)\n",
        "  logit_BoW_1620 = LogisticRegression(C=i)\n",
        "\n",
        "\n",
        "  #Fits the model according to the given training data.\n",
        "  logit_BoW_16.fit(X_train_BagOfWords16, y_train16) \n",
        "  logit_BoW_20.fit(X_train_BagOfWords20, y_train20) \n",
        "  logit_BoW_1620.fit(X_train_BagOfWords1620, y_train1620) \n",
        "\n",
        "\n",
        "  #Predict test set using logistic regression and a given C\n",
        "  acc_table_test16.append(logit_BoW_16.score(X_test_BagOfWords16, y_test16))\n",
        "  acc_table_train16.append(logit_BoW_16.score(X_train_BagOfWords16, y_train16))\n",
        "  acc_table_test20.append(logit_BoW_20.score(X_test_BagOfWords20, y_test20))\n",
        "  acc_table_train20.append(logit_BoW_20.score(X_train_BagOfWords20, y_train20))\n",
        "  acc_table_test1620.append(logit_BoW_1620.score(X_test_BagOfWords1620, y_test1620))\n",
        "  acc_table_train1620.append(logit_BoW_1620.score(X_train_BagOfWords1620, y_train1620))\n",
        "\n",
        "axs[0].plot(param_range, acc_table_train16, label=\"Train score\", color=\"g\")\n",
        "axs[0].plot(param_range, acc_table_test16, label=\"Test score\", color=\"r\")\n",
        "axs[0].set_title('In 2016')\n",
        "axs[1].plot(param_range, acc_table_train20, label=\"Train score\", color=\"g\")\n",
        "axs[1].plot(param_range, acc_table_test20, label=\"Test score\", color=\"r\")\n",
        "axs[1].set_title('In 2020')\n",
        "axs[2].plot(param_range, acc_table_train1620, label=\"Train score\", color=\"g\")\n",
        "axs[2].plot(param_range, acc_table_test1620, label=\"Test score\", color=\"r\")\n",
        "axs[2].set_title('2016 + 2020')\n",
        "fig.suptitle('Accuracy of the Bag of Words model in year 2020 using the logistic regression')\n",
        "axs[0].set(xlabel='C value', ylabel='Accuracy')\n",
        "axs[1].set(xlabel='C value')\n",
        "axs[2].set(xlabel='C value')\n",
        "fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "axs[0].grid()\n",
        "axs[1].grid()\n",
        "axs[2].grid()\n",
        "\n",
        "\n",
        "\n",
        "myList = [\n",
        "    [\"Year\",\"Machine Learning method\",\"Classifier\",\"Testset accuracy C=1\",],\n",
        "    [\"2016\", \"Bag of Words\", \"Logistic Regression\", round(logit_BoW_16.score(X_test_BagOfWords16, y_test16)*100,3)],\n",
        "    [\"2020\", \"Bag of Words\", \"Logistic Regression\", round(logit_BoW_20.score(X_test_BagOfWords20, y_test20)*100,3)],\n",
        "    [\"2016+2020\", \"Bag of Words\", \"Logistic Regression\", round(logit_BoW_1620.score(X_test_BagOfWords1620, y_test1620)*100,3)]\n",
        "\n",
        "]\n",
        "\n",
        "TableIt.printTable(myList, useFieldNames=True)\n",
        "print('')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DukLcEF6y2B9"
      },
      "source": [
        "# Calculate mean and standard deviation for training set scores\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "\n",
        "# Calculate mean and standard deviation for test set scores\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Create plot of Standard Deviation of Accuracy Score\n",
        "plt.plot(param_range, train_std, label=\"Training Deviation\", color=\"r\")\n",
        "plt.plot(param_range, test_std, label=\"validation Deviation\", color=\"g\")\n",
        "plt.title(\"Validation Curve showing Standard Deviation With Logistic Regression\")\n",
        "plt.xlabel(\"C values\")\n",
        "plt.ylabel(\"Standard Deviation\")\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=\"best\")\n",
        "plt.savefig (\"figure1a\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6VoKzEkKdaF"
      },
      "source": [
        "N-Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAbBuXZ6s9gO"
      },
      "source": [
        "# How well does the N-Gram training data predict the test\n",
        "# How well does the Bag of Words training data predict the test data\n",
        "acc_table_train16 = []\n",
        "acc_table_test16 = []\n",
        "acc_table_train20 = []\n",
        "acc_table_test20 = []\n",
        "acc_table_train1620 = []\n",
        "acc_table_test1620 = []\n",
        "param_range= [0.001,0.01,0.05,0.1,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8,2.0,2.5,3,3.5,4,5,6,7,8,9,10]\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, sh¢¢¢arey='row', figsize=(22.5,8))\n",
        "for i in param_range:\n",
        "  #Apply logistic regression model to training data\n",
        "  logit_NG_16 = LogisticRegression(C=i)\n",
        "  logit_NG_20 = LogisticRegression(C=i)\n",
        "  logit_NG_1620 = LogisticRegression(C=i)\n",
        "\n",
        "\n",
        "  #Fits the model according to the given training data.\n",
        "  logit_NG_16.fit(X_train_N_Gram16, y_train16) \n",
        "  logit_NG_20.fit(X_train_N_Gram20, y_train20) \n",
        "  logit_NG_1620.fit(X_train_N_Gram1620, y_train1620) \n",
        "\n",
        "\n",
        "  #Predict test set using logistic regression and a given C\n",
        "  acc_table_test16.append(logit_NG_16.score(X_test_N_Gram16, y_test16))\n",
        "  acc_table_train16.append(logit_NG_16.score(X_train_N_Gram16, y_train16))\n",
        "  acc_table_test20.append(logit_NG_20.score(X_test_N_Gram20, y_test20))\n",
        "  acc_table_train20.append(logit_NG_20.score(X_train_N_Gram20, y_train20))\n",
        "  acc_table_test1620.append(logit_NG_1620.score(X_test_N_Gram1620, y_test1620))\n",
        "  acc_table_train1620.append(logit_NG_1620.score(X_train_N_Gram1620, y_train1620))\n",
        "\n",
        "axs[0].plot(param_range, acc_table_train16, label=\"Train score\", color=\"g\")\n",
        "axs[0].plot(param_range, acc_table_test16, label=\"Test score\", color=\"r\")\n",
        "axs[0].set_title('In 2016')\n",
        "axs[1].plot(param_range, acc_table_train20, label=\"Train score\", color=\"g\")\n",
        "axs[1].plot(param_range, acc_table_test20, label=\"Test score\", color=\"r\")\n",
        "axs[1].set_title('In 2020')\n",
        "axs[2].plot(param_range, acc_table_train1620, label=\"Train score\", color=\"g\")\n",
        "axs[2].plot(param_range, acc_table_test1620, label=\"Test score\", color=\"r\")\n",
        "axs[2].set_title('2016 + 2020')\n",
        "fig.suptitle('Accuracy of the N-Gram model in year 2020 using the logistic regression')\n",
        "axs[0].set(xlabel='C value', ylabel='Accuracy')\n",
        "axs[1].set(xlabel='C value')\n",
        "axs[2].set(xlabel='C value')\n",
        "fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "axs[0].grid()\n",
        "axs[1].grid()\n",
        "axs[2].grid()\n",
        "\n",
        "myList = [\n",
        "    [\"Year\",\"Machine Learning method\",\"Classifier\",\"Testset accuracy C=1\",],\n",
        "    [\"2016\", \"N-Gram\", \"Logistic Regression\", round(logit_NG_16.score(X_test_N_Gram16, y_test16)*100,3)],\n",
        "    [\"2020\", \"N-Gram\", \"Logistic Regression\", round(logit_NG_20.score(X_test_N_Gram20, y_test20)*100,3)],\n",
        "    [\"2016+2020\", \"N-Gram\", \"Logistic Regression\", round(logit_NG_1620.score(X_test_N_Gram1620, y_test1620)*100,3)]\n",
        "\n",
        "]\n",
        "\n",
        "TableIt.printTable(myList, useFieldNames=True)\n",
        "print('')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAzNDe-R9M4r"
      },
      "source": [
        "Logistic Regression is used when the dependent variable (target) is categorical.\n",
        "Above, we are using a Binary Logistic Regression.\n",
        "The categorical response has only two 2 possible outcomes: Democrate or Republican."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZsldGUQAxRB"
      },
      "source": [
        "###Confusion Matrix\n",
        "\n",
        "Compute confusion matrix to evaluate the accuracy of a classification.\n",
        "\n",
        "By definition a confusion matrix C is such that Cij is equal to the number of observations known to be in group i and predicted to be in group j.\n",
        "\n",
        "Thus in our binary classification, the count of true Republican tweets is C[0][0] and false Republican tweets is C[1][0]. Likewise, the count of true Democrat tweets is C[0][1], and false Democrat tweets C[1][1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4_Mua4Tuvs1",
        "outputId": "d337155d-d3fc-465f-af2e-4d4babaf32e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pred_lr_bow16 = logit_BoW_16.predict(X_test_BagOfWords16)\n",
        "pred_lr_bow20 = logit_BoW_20.predict(X_test_BagOfWords20)\n",
        "pred_lr_bow1620 = logit_BoW_1620.predict(X_test_BagOfWords1620)\n",
        "\n",
        "pred_lr_ng16 = logit_NG_16.predict(X_test_N_Gram16)\n",
        "pred_lr_ng20 = logit_NG_20.predict(X_test_N_Gram20)\n",
        "pred_lr_ng1620 = logit_NG_1620.predict(X_test_N_Gram1620)\n",
        "\n",
        "\n",
        "confB16 = confusion_matrix(y_test16, pred_lr_bow16)\n",
        "confB20 = confusion_matrix(y_test20, pred_lr_bow20)\n",
        "confB1620 = confusion_matrix(y_test1620, pred_lr_bow1620)\n",
        "confNG16 = confusion_matrix(y_test16, pred_lr_ng16)\n",
        "confNG20 = confusion_matrix(y_test20, pred_lr_ng20)\n",
        "confNG1620 = confusion_matrix(y_test1620, pred_lr_ng1620)\n",
        "\n",
        "\n",
        "print(\"Confusion matrix\")\n",
        "myList = [\n",
        "    [\"Year\",\"ML method\",\"Classifier\",\"True Democrat\", \"False Republican\",\"False Democrat\", \"True Republican\"],\n",
        "    [\"2016\", \"Bag of Words\", \"Log Reg\", confB16[0][0], confB16[0][1], confB16[1][0], confB16[1][1]],\n",
        "    [\"2016\", \"N-Gram\", \"Log Reg\", confNG16[0][0], confNG16[0][1], confNG16[1][0], confNG16[1][1]],\n",
        "    [\"2020\", \"Bag of Words\", \"Log Reg\", confB20[0][0], confB20[0][1], confB20[1][0], confB20[1][1]],\n",
        "    [\"2020\", \"N-Gram\", \"Log Reg\", confNG20[0][0], confNG20[0][1], confNG20[1][0], confNG20[1][1]],\n",
        "    [\"2016+2020\", \"Bag of Words\", \"Log Reg\", confB1620[0][0], confB1620[0][1], confB1620[1][0], confB1620[1][1]],\n",
        "    [\"2016+2020\", \"N-Gram\", \"Log Reg\", confNG1620[0][0], confNG1620[0][1], confNG1620[1][0], confNG1620[1][1]]\n",
        "]\n",
        "\n",
        "TableIt.printTable(myList, useFieldNames=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Year             | ML method        | Classifier       | True Democrat    | False Republican | False Democrat   | True Republican  |\n",
            "+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
            "| 2016             | Bag of Words     | Log Reg          | 691              | 51               | 63               | 582              |\n",
            "| 2016             | N-Gram           | Log Reg          | 700              | 42               | 50               | 595              |\n",
            "| 2020             | Bag of Words     | Log Reg          | 485              | 35               | 29               | 598              |\n",
            "| 2020             | N-Gram           | Log Reg          | 488              | 32               | 27               | 600              |\n",
            "| 2016+2020        | Bag of Words     | Log Reg          | 1180             | 108              | 94               | 1151             |\n",
            "| 2016+2020        | N-Gram           | Log Reg          | 1202             | 86               | 77               | 1168             |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}