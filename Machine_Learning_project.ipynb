{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarjerw/TDT4173-project-group6/blob/main/Machine_Learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-InharmhPF7W"
      },
      "source": [
        "Political party analysis program that parses the tweets fetched from Twitter using Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDX2HpAbVCwU"
      },
      "source": [
        "###### IMPORT THE LIBRARIES  #########\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYZX69rc97WW"
      },
      "source": [
        "#Candidate twitter accounts\n",
        "Rep2012 = [\"MittRomney\",\"RickSantorum\",\"RonPaul\",\"newtgingrich\",\"BuddyRoemer\",\"SecretaryPerry\",\"MicheleBachmann\"]\n",
        "Rep2016 = [\"realDonaldTrump\",\"tedcruz\",\"marcorubio\",\"JohnKasich\",\"SecretaryCarson\",\"JebBush\",\"RandPaul\",\"GovMikeHuckabee\",\"CarlyFiorina\",\"GovChristie\",\"gov_gilmore\"]\n",
        "Rep2020 = [\"realDonaldTrump\"]\n",
        "Dem2012 = [\"BarackObama\"]\n",
        "Dem2016 =  [\"MartinOMalley\",\"BernieSanders\",\"HillaryClinton\"]\n",
        "Dem2020 = [\"JoeBiden\",\"AndrewYang\",\"BernieSanders\",\"TulsiGabbard\",\"ewarren\",\"MikeBloomberg\",\"amyklobuchar\",\"PeteButtigieg\",\"TomSteyer\",\"DevalPatrick\",\"MichaelBennet\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZom6A0JPscH"
      },
      "source": [
        "##Retrieve data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuCY4Az8GMCu"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvhRzsKoGpGc"
      },
      "source": [
        "#retrieves data\n",
        "wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/18a7-8eM-rcraKagnXPpCmYbC4uwWwBVq_iF_IwozYos/edit#gid=787454996')\n",
        "sheet = wb.worksheet('BarackObama')\n",
        "data = sheet.get_all_values()\n",
        "df_all = pd.DataFrame(data)\n",
        "df_all.columns = df_all.iloc[0]\n",
        "df_all = df_all.iloc[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxoc5_GtOjF-",
        "outputId": "2c2a4c3d-c622-49e7-f8bf-f2839717fd87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "#shuffles data\n",
        "df_all = df_all.sample(frac = 1)\n",
        "df_all.iloc[20:30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Candidate</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Year</th>\n",
              "      <th>Party</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8656</th>\n",
              "      <td>DonaldTrump</td>\n",
              "      <td>Thank you Mr. &amp;amp; Mrs. @TomBarrackJr for the...</td>\n",
              "      <td>2016</td>\n",
              "      <td>Republican</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8887</th>\n",
              "      <td>DonaldTrump</td>\n",
              "      <td>Thank you Fort Wayne Indiana!#Trump2016 #INPri...</td>\n",
              "      <td>2016</td>\n",
              "      <td>Republican</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6547</th>\n",
              "      <td>BernieSanders</td>\n",
              "      <td>Wisconsin — don't know where to vote for Berni...</td>\n",
              "      <td>2016</td>\n",
              "      <td>Democrat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7881</th>\n",
              "      <td>DonaldTrump</td>\n",
              "      <td>RT @Avik: So @YouTube just took down a June 23...</td>\n",
              "      <td>2020</td>\n",
              "      <td>Republican</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7037</th>\n",
              "      <td>DonaldTrump</td>\n",
              "      <td>The GREAT Bobby Bowden one of the best coaches...</td>\n",
              "      <td>2020</td>\n",
              "      <td>Republican</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7266</th>\n",
              "      <td>DonaldTrump</td>\n",
              "      <td>HAPPY COLUMBUS DAY TO ALL!</td>\n",
              "      <td>2020</td>\n",
              "      <td>Republican</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4973</th>\n",
              "      <td>HillaryClinton</td>\n",
              "      <td>Our teachers deserve more than just a pat on t...</td>\n",
              "      <td>2016</td>\n",
              "      <td>Democrat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8274</th>\n",
              "      <td>DonaldTrump</td>\n",
              "      <td>Congratulations to Dustin Johnson @DJohnsonPGA...</td>\n",
              "      <td>2020</td>\n",
              "      <td>Republican</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2429</th>\n",
              "      <td>HillaryClinton</td>\n",
              "      <td>\"You don't grade the presidency on a curve.\" —...</td>\n",
              "      <td>2016</td>\n",
              "      <td>Democrat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9619</th>\n",
              "      <td>DonaldTrump</td>\n",
              "      <td>Remember Cruz and Bush gave us Roberts who uph...</td>\n",
              "      <td>2016</td>\n",
              "      <td>Republican</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "0          Candidate  ...       Party\n",
              "8656     DonaldTrump  ...  Republican\n",
              "8887     DonaldTrump  ...  Republican\n",
              "6547   BernieSanders  ...    Democrat\n",
              "7881     DonaldTrump  ...  Republican\n",
              "7037     DonaldTrump  ...  Republican\n",
              "7266     DonaldTrump  ...  Republican\n",
              "4973  HillaryClinton  ...    Democrat\n",
              "8274     DonaldTrump  ...  Republican\n",
              "2429  HillaryClinton  ...    Democrat\n",
              "9619     DonaldTrump  ...  Republican\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJnpSQr2eudp",
        "outputId": "8bc11c75-e167-4e74-bc1a-154a6eb76cc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#shows the number of duplicated tweets\n",
        "df_all = df_all.drop_duplicates() #removes all duplicates\n",
        "df_all.duplicated().value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    9782\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmDRqfADfdIZ",
        "outputId": "a3b7b40c-44ca-4769-8b2c-b6eb748ad5e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#counts the number of democrate tweets and republican tweets\n",
        "df_all.Party.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Democrat      6761\n",
              "Republican    3021\n",
              "Name: Party, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yWOpVu5OJwB"
      },
      "source": [
        "##Sentiment Analysis using Bag-of-Words & N-Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjye1aYdoIaN"
      },
      "source": [
        "#import needed libraries\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crtXnwhoamMy"
      },
      "source": [
        "###Most common words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgb8c93vdS15"
      },
      "source": [
        "####Most common republican words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quWu2RtWa5L2",
        "outputId": "2c40a541-a5cc-4ac9-9ffc-0de54615f68d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_rep = df_all.loc[df_all['Party'] == 'Republican']\n",
        "\n",
        "rep_BagOfWords_vectorizer = CountVectorizer(max_df=0.35)\n",
        "data_rep_BagOfWords = rep_BagOfWords_vectorizer.fit_transform(data_rep.Tweet)\n",
        "\n",
        "freqs = zip(rep_BagOfWords_vectorizer.get_feature_names(), data_rep_BagOfWords.sum(axis=0).tolist()[0])    \n",
        "freqs_list = list(freqs)\n",
        "sorted_freps = sorted(freqs_list, key=lambda x: -x[1])\n",
        "for i in range(30):\n",
        "  print(sorted_freps[i+100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('them', 84)\n",
            "('bad', 83)\n",
            "('because', 82)\n",
            "('years', 82)\n",
            "('him', 81)\n",
            "('realdonaldtrump', 81)\n",
            "('way', 81)\n",
            "('crooked', 80)\n",
            "('endorsement', 80)\n",
            "('tonight', 79)\n",
            "('said', 78)\n",
            "('her', 77)\n",
            "('carolina', 76)\n",
            "('pennsylvania', 76)\n",
            "('state', 76)\n",
            "('time', 76)\n",
            "('were', 75)\n",
            "('china', 74)\n",
            "('enjoy', 74)\n",
            "('how', 74)\n",
            "('there', 74)\n",
            "('want', 74)\n",
            "('against', 73)\n",
            "('left', 73)\n",
            "('sleepy', 71)\n",
            "('florida', 70)\n",
            "('day', 69)\n",
            "('don', 69)\n",
            "('down', 69)\n",
            "('go', 67)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRbJF39KfVNi"
      },
      "source": [
        "####Most common democrat words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pP4wVdHfYzI",
        "outputId": "dd5b0334-69e5-4297-c739-6f418531347a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_dem = df_all.loc[df_all['Party'] == 'Democrat']\n",
        "\n",
        "dem_BagOfWords_vectorizer = CountVectorizer(max_df=0.35)\n",
        "data_dem_BagOfWords = dem_BagOfWords_vectorizer.fit_transform(data_dem.Tweet)\n",
        "\n",
        "freqs_dem = zip(dem_BagOfWords_vectorizer.get_feature_names(), data_dem_BagOfWords.sum(axis=0).tolist()[0])    \n",
        "freqs_list = list(freqs_dem)\n",
        "sorted_freps = sorted(freqs_list, key=lambda x: -x[1])\n",
        "for i in range(30):\n",
        "  print(sorted_freps[i+100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('berniesanders', 200)\n",
            "('want', 200)\n",
            "('twitter', 199)\n",
            "('would', 196)\n",
            "('been', 194)\n",
            "('crisis', 193)\n",
            "('she', 193)\n",
            "('go', 191)\n",
            "('plan', 190)\n",
            "('never', 189)\n",
            "('better', 188)\n",
            "('change', 185)\n",
            "('economy', 177)\n",
            "('amp', 176)\n",
            "('clinton', 173)\n",
            "('fight', 173)\n",
            "('because', 170)\n",
            "('first', 170)\n",
            "('world', 170)\n",
            "('live', 169)\n",
            "('last', 168)\n",
            "('covid', 164)\n",
            "('election', 161)\n",
            "('millions', 159)\n",
            "('too', 159)\n",
            "('only', 158)\n",
            "('everyone', 156)\n",
            "('join', 156)\n",
            "('19', 155)\n",
            "('million', 155)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyygWCNVCal3"
      },
      "source": [
        "###Splitting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPAiM8lDhDKt",
        "outputId": "e24680a0-755d-42ea-922f-07266f7c621b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Splits dataset into Training data and Test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_all.Tweet, df_all.Party, test_size=0.3)\n",
        "\n",
        "print(\"Democratic republican ratio in Training data:\\n\"+str(y_train.value_counts())+\"\\n\")\n",
        "print(\"Democratic republican ratio in Test data:\\n\"+str(y_test.value_counts()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Democratic republican ratio in Training data:\n",
            "Democrat      4725\n",
            "Republican    2122\n",
            "Name: Party, dtype: int64\n",
            "\n",
            "Democratic republican ratio in Test data:\n",
            "Democrat      2036\n",
            "Republican     899\n",
            "Name: Party, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bob8NcLkoB0C"
      },
      "source": [
        "####Creating Bag-of-Words Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SFhnV7boofN"
      },
      "source": [
        "# Create Bag of words\n",
        "BagOfWords_vectorizer = CountVectorizer(max_df=0.35)\n",
        "X_train_BagOfWords = BagOfWords_vectorizer.fit_transform(X_train)\n",
        "X_test_BagOfWords = BagOfWords_vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCySJUzdDZat"
      },
      "source": [
        "####Creating N-Gram Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk8tS1kZDkUM"
      },
      "source": [
        "# Create Bag of words\n",
        "N_Gram_vectorizer = CountVectorizer(ngram_range=(1,3), max_df=0.35) #max_df=0.8 removes all words that are present more than 80% of the time\n",
        "X_train_N_Gram = N_Gram_vectorizer.fit_transform(X_train)\n",
        "X_test_N_Gram = N_Gram_vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHmsK1EdG6Mf",
        "outputId": "5344cb60-a62b-4167-c8d4-441f3ba6d96b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "######### MOST COMMON WORDS IN ENTIRE DATASET ###########################\n",
        "matrix = N_Gram_vectorizer.fit_transform(X_train)\n",
        "freqs = zip(N_Gram_vectorizer.get_feature_names(), matrix.sum(axis=0).tolist()[0])    \n",
        "# sort from largest to smallest\n",
        "print(\"most frequent words and word combinations in N-Gram:\\n\", sorted(freqs, key=lambda x: -x[1]))\n",
        "\n",
        "######### SEE SPECIFIC WORDS AND THEIR RESPECTIVE ID ##################\n",
        "\n",
        "#BagOfWords_vectorizer.vocabulary_\n",
        "#N_Gram_vectorizer.vocabulary_ # uncomment to compare\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex0MwkPk2Psp"
      },
      "source": [
        "###Performing cross validation\n",
        "\n",
        "A key challenge with machine learning, is that we can’t know how well our model will perform on new data until we actually test it.\n",
        "\n",
        "To address this, we can split our initial dataset into separate training and test subsets.\n",
        "\n",
        "There are different types of Cross Validation Techniques but the overall concept remains the same:\n",
        "\n",
        "*   To partition the data into a number of subsets (given as argument for cv in cross_val_score below)\n",
        "*   Hold out a set at a time and train the model on remaining set\n",
        "*   Test model on hold out set\n",
        "*   Repeat the process for each subset of the dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A cross validation is done in the code segment below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt-a2v4Bk3c2",
        "outputId": "360c328d-0ffa-4bd8-c554-bc2a36cbecbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross-validation is a statistical method used to estimate the skill of machine learning models.\n",
        "scores_BagOfWords = cross_val_score(LogisticRegression(), X_train_BagOfWords, y_train, cv=5)\n",
        "scores_N_Gram = cross_val_score(LogisticRegression(), X_train_N_Gram, y_train, cv=5)\n",
        "\n",
        "# Returns an array of scores of the estimator for each run of the cross validation.\n",
        "print(\"Bag of Words, cross validation scores:\",scores_BagOfWords) \n",
        "print(\"N-Gram, cross validation scores:\",scores_N_Gram) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag of Words, cross validation scores: [0.94160584 0.9379562  0.93352812 0.94010226 0.93498904]\n",
            "N-Gram, cross validation scores: [0.94671533 0.9379562  0.93791088 0.93791088 0.93791088]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDIaIGzRCGSX"
      },
      "source": [
        "###Data Prediction\n",
        "\n",
        "How well does the training data predict the test data?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-zxaKFaKWUQ"
      },
      "source": [
        "Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64yVS3enKUiT",
        "outputId": "9a986cb7-8045-4dda-e7d3-719dbdc923d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# How well does the Bag of Words training data predict the test data\n",
        "logistic_reg_BagOfWords = LogisticRegression()\n",
        "logistic_reg_BagOfWords.fit(X_train_BagOfWords, y_train) #Fits the model according to the given training data.\n",
        "print(\"The logistic regression, \\ncreated and trained on Training data predicts\", round(logistic_reg_BagOfWords.score(X_train_BagOfWords, y_train)*100,3), \"% og the Training dataset correctly.\")\n",
        "print(\"More importantly, it predicts\", round(logistic_reg_BagOfWords.score(X_test_BagOfWords, y_test)*100,3), \"% og the Test dataset correctly!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The logistic regression, \n",
            "created and trained on Training data predicts 99.576 % og the Training dataset correctly.\n",
            "More importantly, it predicts 94.48 % og the Test dataset correctly!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6VoKzEkKdaF"
      },
      "source": [
        "N-Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAbBuXZ6s9gO",
        "outputId": "e3c23f2f-8ff4-43c9-ad0b-d61497537bc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# How well does the N-Gram training data predict the test data\n",
        "logistic_reg_N_Gram = LogisticRegression()\n",
        "logistic_reg_N_Gram.fit(X_train_N_Gram, y_train) #Fits the model according to the given training data.\n",
        "print(\"The logistic regression, \\ncreated and trained on Training data predicts\", round(logistic_reg_N_Gram.score(X_train_N_Gram, y_train)*100,3), \"% og the Training dataset correctly.\")\n",
        "print(\"More importantly, it predicts\", round(logistic_reg_N_Gram.score(X_test_N_Gram, y_test)*100,3), \"% og the Test dataset correctly!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The logistic regression, \n",
            "created and trained on Training data predicts 99.971 % og the Training dataset correctly.\n",
            "More importantly, it predicts 95.537 % og the Test dataset correctly!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAzNDe-R9M4r"
      },
      "source": [
        "Logistic Regression is used when the dependent variable (target) is categorical.\n",
        "Above, we are using a Binary Logistic Regression.\n",
        "The categorical response has only two 2 possible outcomes: Democrate or Republican."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZsldGUQAxRB"
      },
      "source": [
        "###Confusion Matrix\n",
        "\n",
        "Compute confusion matrix to evaluate the accuracy of a classification.\n",
        "\n",
        "By definition a confusion matrix C is such that Cij is equal to the number of observations known to be in group i and predicted to be in group j.\n",
        "\n",
        "Thus in our binary classification, the count of true Republican tweets is C[0][0] and false Republican tweets is C[1][0]. Likewise, the count of true Democrat tweets is C[0][1], and false Democrat tweets C[1][1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4_Mua4Tuvs1",
        "outputId": "60b11183-0318-4e42-8f42-0183e41c6d57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pred_logreg_BagOfWords = logistic_reg_BagOfWords.predict(X_test_BagOfWords)\n",
        "confusionBoW = confusion_matrix(y_test, pred_logreg_BagOfWords)\n",
        "print(\"Bag of Words:\\n\", confusionBoW)\n",
        "\n",
        "pred_logreg_N_Gram = logistic_reg_N_Gram.predict(X_test_N_Gram)\n",
        "confusionNG = confusion_matrix(y_test, pred_logreg_N_Gram)\n",
        "print(\"N-Gram:\\n\", confusionNG)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag of Words:\n",
            " [[1993   43]\n",
            " [ 119  780]]\n",
            "N-Gram:\n",
            " [[1999   37]\n",
            " [  94  805]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGwQdySMZfSl"
      },
      "source": [
        "#Sentiment Analysis using Convolution Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNbMkTsoWory"
      },
      "source": [
        "https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSOnY5y4-V-q",
        "outputId": "4c986804-eaaf-449b-ecb8-327090ba1878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#data cleaning \n",
        "import re\n",
        "import string\n",
        "\n",
        "def remove_punct(text):\n",
        "    text_nopunct = ''\n",
        "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
        "    return text_nopunct\n",
        "df_all['Clean_Tweet'] = df_all['Tweet'].apply(lambda x: remove_punct(x))\n",
        "\n",
        "#we tokenize the comments by using NLTK’s word_tokenize. e.g, \"hello world\" -> [\"hello\",\"world\"]\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "tokens = [word_tokenize(sen) for sen in df_all.Clean_Tweet]\n",
        "\n",
        "#set to lower case\n",
        "def lower_token(tokens): \n",
        "    return [w.lower() for w in tokens]    \n",
        "    \n",
        "lower_tokens = [lower_token(token) for token in tokens]\n",
        "\n",
        "#remove stopwords. Filter out certain words\n",
        "nltk.download('stopwords')\n",
        "stoplist = stopwords.words('english') #words to filter out\n",
        "\n",
        "def removeStopWords(tokens): \n",
        "    return [word for word in tokens if word not in stoplist]\n",
        "filtered_words = [removeStopWords(sen) for sen in lower_tokens]\n",
        "\n",
        "df_all['Tweet_Final'] = [' '.join(sen) for sen in filtered_words] #sets the final text in df\n",
        "df_all['tokens'] = filtered_words #sets the final tokens in the final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMz9us5n_qcb",
        "outputId": "9bfe6433-46c2-4776-d820-c3bc0e5c31ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "source": [
        "pos = []\n",
        "neg = []\n",
        "for l in df_all.Party: #define as positive or negative (label for supervised training)\n",
        "    if l == \"Democrat\":\n",
        "        pos.append(1)\n",
        "        neg.append(0)\n",
        "    elif l == \"Republican\":\n",
        "        pos.append(0)\n",
        "        neg.append(1)\n",
        "df_all['Pos']= pos\n",
        "df_all['Neg']= neg\n",
        "\n",
        "df_all = df_all[['Tweet_Final', 'tokens', 'Party', 'Pos', 'Neg']]\n",
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet_Final</th>\n",
              "      <th>tokens</th>\n",
              "      <th>Party</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3582</th>\n",
              "      <td>thank ddlovato describing experience dealing m...</td>\n",
              "      <td>[thank, ddlovato, describing, experience, deal...</td>\n",
              "      <td>Democrat</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>forced move rally tonight fayetteville north c...</td>\n",
              "      <td>[forced, move, rally, tonight, fayetteville, n...</td>\n",
              "      <td>Republican</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>’ always believed ’ best ’ one america come to...</td>\n",
              "      <td>[’, always, believed, ’, best, ’, one, america...</td>\n",
              "      <td>Democrat</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1370</th>\n",
              "      <td>donald trump responsible covid19 bear full res...</td>\n",
              "      <td>[donald, trump, responsible, covid19, bear, fu...</td>\n",
              "      <td>Democrat</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>unlike current president always choose science...</td>\n",
              "      <td>[unlike, current, president, always, choose, s...</td>\n",
              "      <td>Democrat</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "0                                           Tweet_Final  ... Neg\n",
              "3582  thank ddlovato describing experience dealing m...  ...   0\n",
              "49    forced move rally tonight fayetteville north c...  ...   1\n",
              "106   ’ always believed ’ best ’ one america come to...  ...   0\n",
              "1370  donald trump responsible covid19 bear full res...  ...   0\n",
              "1803  unlike current president always choose science...  ...   0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emwzR0rACnd_"
      },
      "source": [
        "# Splits dataset into Training data and Test data\n",
        "\n",
        "data_train, data_test = train_test_split(df_all, \n",
        "                                         test_size=0.10, \n",
        "                                         random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LjIsuCiDBmD",
        "outputId": "33c62419-04cf-40a0-ee80-3b22f9a4f362",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens] #all words used in training data\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]] #finds max lenght of training data\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words))) #vocabulary of words in training data\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
        "\n",
        "#do the same for the test data\n",
        "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115085 words total, with a vocabulary size of 18081\n",
            "Max sentence length is 46\n",
            "13066 words total, with a vocabulary size of 4529\n",
            "Max sentence length is 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FUF_qiHEFsW"
      },
      "source": [
        "#creating a word2vec using reddit world news dataset https://www.kaggle.com/rootuser/worldnews-on-reddit\n",
        "#what is word2vec? https://code.google.com/archive/p/word2vec/ , transform word to vector\n",
        "import gensim.models\n",
        "from gensim.models import Word2Vec, KeyedVectors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIdKk9qlOXnz"
      },
      "source": [
        "import io\n",
        "reddit_df = pd.read_csv('reddit_worldnews_start_to_2016-11-22.csv')\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "reddit_df.head(10)\n",
        "\n",
        "newsTitles  = reddit_df['title'].values\n",
        "newsVec = [nltk.word_tokenize(title) for title in newsTitles]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwvIHTk1ndDd"
      },
      "source": [
        "word2vec_dim = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U21BvMD4R9Jj"
      },
      "source": [
        "\n",
        "word2vec = Word2Vec(newsVec,min_count=1,size= word2vec_dim) #creating the word2vec model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M08B089PSlEp",
        "outputId": "5641d602-320a-4573-936b-20de6604cc97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#testing the word2vec\n",
        "word2vec.most_similar(['McKinsey'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Handelsblatt', 0.7613131403923035),\n",
              " ('Trends', 0.7517812252044678),\n",
              " ('Yeast', 0.7414613962173462),\n",
              " ('Inverted', 0.7381319403648376),\n",
              " ('Trend', 0.7376179695129395),\n",
              " ('YouTubers', 0.736943244934082),\n",
              " ('Mapped', 0.7368432283401489),\n",
              " ('Hotspots', 0.7347134351730347),\n",
              " ('Rankings', 0.7344036102294922),\n",
              " ('Krispy', 0.7331280708312988)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv41mfbBmJJh"
      },
      "source": [
        "#word2vec functions\n",
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=word2vec_dim):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A70lBE3LTvgw",
        "outputId": "68f3c00a-b6bd-4d47-df7e-216a0875e926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(data_train['Tweet_Final'].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train['Tweet_Final'].tolist())\n",
        "train_word_index = tokenizer.word_index\n",
        "testing_sequences = tokenizer.texts_to_sequences(data_test['Tweet_Final'].tolist())\n",
        "test_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "train_cnn_data = pad_sequences(training_sequences, \n",
        "                               maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_cnn_data = pad_sequences(testing_sequences, \n",
        "                               maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18081 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu6sJOpyh9hZ",
        "outputId": "ec3e7949-44f1-4dde-fcac-2046229e36f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EMBEDDING_DIM = word2vec_dim\n",
        "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0DzjtXFV_kE",
        "outputId": "73d680a1-c8a1-4a7a-c9be-e3e5cdd45ff8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
        " EMBEDDING_DIM))\n",
        "for word,index in train_word_index.items():\n",
        " train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18082, 300)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr1vaRAjWlL4"
      },
      "source": [
        "#defining a CNN\n",
        "\n",
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        " \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    convs = []\n",
        "    filter_sizes = [2,3,4,5,6]\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=200, \n",
        "                        kernel_size=filter_size, \n",
        "                        activation='relu')(embedded_sequences)\n",
        "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "        convs.append(l_pool)\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "    x = Dropout(0.1)(l_merge)  \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WPOub-1XEmE",
        "outputId": "122b23b1-e6d3-4842-8dd4-0b7a6db6a0f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from gensim import models\n",
        "from keras.models import Model\n",
        "\n",
        "label_names = ['Pos', 'Neg']\n",
        "model = ConvNet(train_embedding_weights, \n",
        "                MAX_SEQUENCE_LENGTH, \n",
        "                len(train_word_index)+1, \n",
        "                EMBEDDING_DIM, \n",
        "                len(list(label_names)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_15\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, 50, 300)      5424600     input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_45 (Conv1D)              (None, 49, 200)      120200      embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_46 (Conv1D)              (None, 48, 200)      180200      embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_47 (Conv1D)              (None, 47, 200)      240200      embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_48 (Conv1D)              (None, 46, 200)      300200      embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_49 (Conv1D)              (None, 45, 200)      360200      embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_45 (Global (None, 200)          0           conv1d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_46 (Global (None, 200)          0           conv1d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_47 (Global (None, 200)          0           conv1d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_48 (Global (None, 200)          0           conv1d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_49 (Global (None, 200)          0           conv1d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 1000)         0           global_max_pooling1d_45[0][0]    \n",
            "                                                                 global_max_pooling1d_46[0][0]    \n",
            "                                                                 global_max_pooling1d_47[0][0]    \n",
            "                                                                 global_max_pooling1d_48[0][0]    \n",
            "                                                                 global_max_pooling1d_49[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 1000)         0           concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 128)          128128      dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 128)          0           dense_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 2)            258         dropout_19[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 6,753,986\n",
            "Trainable params: 1,329,386\n",
            "Non-trainable params: 5,424,600\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf-7eGWFXwFD",
        "outputId": "2be4e55e-3335-468c-efab-14763048c9d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#training the CNN\n",
        "y_train = data_train[label_names].values\n",
        "x_train = train_cnn_data\n",
        "\n",
        "num_epochs = 3 #number of times data is passed through\n",
        "batch_size = 200#number of data points passed through at once\n",
        "hist = model.fit(x_train, \n",
        "                 y_train, \n",
        "                 epochs=num_epochs, \n",
        "                 validation_split=0.1, \n",
        "                 shuffle=True, \n",
        "                 batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "41/41 [==============================] - 38s 916ms/step - loss: 0.1725 - acc: 0.9390 - val_loss: 0.2498 - val_acc: 0.8969\n",
            "Epoch 2/3\n",
            "41/41 [==============================] - 38s 918ms/step - loss: 0.1148 - acc: 0.9616 - val_loss: 0.2752 - val_acc: 0.8890\n",
            "Epoch 3/3\n",
            "41/41 [==============================] - 38s 918ms/step - loss: 0.0941 - acc: 0.9680 - val_loss: 0.2589 - val_acc: 0.9002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ_Zf3J7YMcn",
        "outputId": "67de56d1-6674-4040-bd57-2110e136d1a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#make prediction on test data using trained CNN\n",
        "predictions = model.predict(test_cnn_data, \n",
        "                            batch_size=300, \n",
        "                            verbose=1)\n",
        "labels = [\"Democrat\", \"Republican\"]\n",
        "prediction_labels=[]\n",
        "\n",
        "for p in predictions:\n",
        "    prediction_labels.append(labels[np.argmax(p)])\n",
        "\n",
        "print(len(predictions))\n",
        "print(data_test.Party.value_counts())\n",
        "sum(data_test.Party==prediction_labels)/len(prediction_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 330ms/step\n",
            "992\n",
            "Democrat      686\n",
            "Republican    306\n",
            "Name: Party, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8981854838709677"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 298
        }
      ]
    }
  ]
}