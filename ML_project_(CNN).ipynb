{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML project (CNN).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5tkWDwAuHOuI",
        "DZom6A0JPscH",
        "CGwQdySMZfSl",
        "qmo9Lc98l8S9",
        "8xkYg8s_Kaeg",
        "AMt6Of0vKfwm",
        "Yd7h0kN5O6nH",
        "2dmgxKKSMbqJ",
        "53Ceno8yL5im",
        "gk4t-8OAMT81",
        "flUEksH4O-3y",
        "tm0BEeooPGAT",
        "9BSF7xHFJuGD"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarjerw/TDT4173-project-group6/blob/main/ML_project_(CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-InharmhPF7W"
      },
      "source": [
        "#Affiliation analysis of political tweets - CNN\n",
        "This is the code used for training a CNN for doing political affiliation classification (democrat or republican) of tweets, and testing it on test data, discussed in the paper *Affiliation analysis of politcal tweets*. CNN is disscussed in section *4.2 Convolutional neural network (CNN)* and *5.2 Convolutional neural network (CNN) results*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tkWDwAuHOuI"
      },
      "source": [
        "##Imported libaries\n",
        "All external libaries used are imported. The main ones are pandas, numpy, sklearn, gensim and keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDX2HpAbVCwU"
      },
      "source": [
        "###### IMPORT THE LIBRARIES  #########\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "#sklearn, used for train_test_split\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#used for data cleaning\n",
        "import re\n",
        "import string\n",
        "\n",
        "#used nltk used for sopwords and tokenizing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#gensim is used to help create the Word2Vec model used\n",
        "import gensim.models\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim import models\n",
        "\n",
        "import io\n",
        "\n",
        "#keras is used for creating the CNN \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZom6A0JPscH"
      },
      "source": [
        "##Data retrival\n",
        "The datasets of tweets are read from csv files and saved in a pandas dataframe. Here you need to define which dataset to use (df_all = tweets2016_df, tweets2020_df or tweets2016_2020_df). These datasets are discussed in section *3 Data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvhRzsKoGpGc"
      },
      "source": [
        "#retrieves data\n",
        "tweets2016_df = pd.read_csv('TweetDatabase_2016.csv')\n",
        "tweets2020_df = pd.read_csv('TweetDatabase_2020.csv')\n",
        "tweets2016_2020_df = tweets2016_df.append(tweets2020_df)\n",
        "df_all = tweets2016_2020_df #set which dataset to use (2016 or 2020,or both)\n",
        "df_all = df_all.drop_duplicates() #removes all duplicates\n",
        "#shuffles data\n",
        "df_all = df_all.sample(frac = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGwQdySMZfSl"
      },
      "source": [
        "##Creating a word embedding using word2vec\n",
        "Here the code is for training the word2vec word embedding which is used for converting words to vectors. Here you need to define the dimensiom of vector for each word (word2vec_dim), currently set to 350. This method is discussed in section *4.2.1 Word embedding, using word2vec* and *5.2.1 Tuning word2vec dimension hyperparameter*.\n",
        "\n",
        "NB: Before running, the dataset 'reddit_worldnews_start_to_2016-11-22.csv' need to be uploaded to colab. It can be accessed here: https://www.kaggle.com/rootuser/worldnews-on-reddit\n",
        "\n",
        "Running this code might take some time, ca. 4 min\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kiqff5_OcYm"
      },
      "source": [
        "reddit_df = pd.read_csv('reddit_worldnews_start_to_2016-11-22.csv') #https://www.kaggle.com/rootuser/worldnews-on-reddit\n",
        "# Dataset is now stored in a Pandas Dataframe \n",
        "\n",
        "#make all words lowercase, as all words in tweets are converted to lowercase \n",
        "reddit_df['title'] = reddit_df['title'].str.lower()\n",
        "\n",
        "newsTitles  = reddit_df['title'].values\n",
        "tweets2016 = tweets2016_df['Tweet'].values\n",
        "tweets2020 = tweets2020_df['Tweet'].values\n",
        "newsTitles = np.concatenate((newsTitles, tweets2016, tweets2020), axis=0) #add tweets data for training the word2vec model\n",
        "newsVec = [nltk.word_tokenize(title) for title in newsTitles]\n",
        "\n",
        "word2vec_dim = 350 #sets the dimension of the vector for each word\n",
        "\n",
        "word2vec = Word2Vec(newsVec,min_count=1,size= word2vec_dim) #creating the word2vec model, using gensim.Word2Vec function with the data from the reddit_worldnews dataset\n",
        "#this model can be used for converting words to vectors! :) \n",
        "\n",
        "#prints timestamp of when last completed\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "print(\"Last updated at =\", current_time, \"word2vec_dim set to:\", word2vec_dim50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmo9Lc98l8S9"
      },
      "source": [
        "##Data preproccesing and data split\n",
        "In this section nessesary data preproccesing (e.g., data cleaning and word tokenizing) is done. The data is also sepe into training- and test data. This is discussed in sections *3.2 Preprocessing* and *3.3 Data split*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xkYg8s_Kaeg"
      },
      "source": [
        "###Data Cleaning\n",
        "Tweet data is cleaned. Include removing punctuations, tokenize words, set words to lowercase, and removing stopwords. This is discussed in section *3.2 Preprocessing*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSOnY5y4-V-q"
      },
      "source": [
        "#removes punctuation for all words\n",
        "def nopunct(inputString):\n",
        "    nopunct = ''\n",
        "    nopunct = re.sub('['+string.punctuation+']', '', inputString)\n",
        "    return nopunct\n",
        "    \n",
        "df_all['Clean_Tweet'] = df_all['Tweet'].apply(lambda x: nopunct(x))\n",
        "\n",
        "#use NLTKâ€™s word_tokenize, which turns sentences into list of words(tokens). e.g, \"hello world\" -> [\"hello\",\"world\"]\n",
        "tokens = [word_tokenize(tweet) for tweet in df_all.Clean_Tweet]\n",
        "\n",
        "#set all words to lower case\n",
        "def lword(tweet): \n",
        "    return [t.lower() for t in tweet]     \n",
        "lower_tweet = [lword(tweet) for tweet in tokens]\n",
        "\n",
        "#remove stopwords. Filter out certain words, using the NLTK's stopwords\n",
        "stopwordList = stopwords.words('english') #words to filter out, stopwords imported from NLTK\n",
        "def delStopWords(tweet): \n",
        "    return [word for word in tweet if word not in stopwordList]\n",
        "filtered_w = [delStopWords(tweet) for tweet in lower_tweet]\n",
        "df_all['Tweet_Final'] = [' '.join(tweet) for tweet in filtered_w] #sets the final tweet into the dataframe\n",
        "df_all['tokens'] = filtered_w #sets the final tokens into the dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMt6Of0vKfwm"
      },
      "source": [
        "###Dataframe modification\n",
        "Added republican (0 or 1) and democrat (0 or 1) in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMz9us5n_qcb"
      },
      "source": [
        "#want to make a variable \"rep\" which is 1 if the candidate is a republican and 0 otherwise. Also want a \"dem\" variable\n",
        "rep = []\n",
        "dem = []\n",
        "for party in df_all.Party: #define as republican or democrat (label for supervised training)\n",
        "    if party == \"Democrat\":\n",
        "        dem.append(1)\n",
        "        rep.append(0)\n",
        "    elif party == \"Republican\":\n",
        "        dem.append(0)\n",
        "        rep.append(1)\n",
        "df_all['rep']= rep\n",
        "df_all['dem']= dem\n",
        "\n",
        "df_all = df_all[['Tweet_Final', 'tokens', 'Party', 'rep', 'dem']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd7h0kN5O6nH"
      },
      "source": [
        "###Data split\n",
        "Data is split into training (80%) and test data (20%). This is discussed in sections *3.3 Data split*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emwzR0rACnd_"
      },
      "source": [
        "test_size = 0.20 #share of data used for testing\n",
        "training_data, test_data = train_test_split(df_all, \n",
        "                                         test_size=test_size, \n",
        "                                         random_state=36)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dmgxKKSMbqJ"
      },
      "source": [
        "###Assigning word to number\n",
        "Each unique word is assigned a number which it can be referenced by"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A70lBE3LTvgw"
      },
      "source": [
        "#finds the number of unique words in the training data\n",
        "training_words = [word for tokens in training_data[\"tokens\"] for word in tokens] #all words used in training data\n",
        "train_vocab_lenght = len(sorted(list(set(training_words)))) #number of unique words in training data\n",
        "\n",
        "#Keras.Tokenizer allows for creating vectors out of text corpus by asigning each unique word a integer \n",
        "tokenizer = Tokenizer(num_words=train_vocab_lenght, lower=True, char_level=False) \n",
        "tokenizer.fit_on_texts(training_data['Tweet_Final'].tolist()) \n",
        "\n",
        "train_seq = tokenizer.texts_to_sequences(training_data['Tweet_Final'].tolist())\n",
        "train_word_index = tokenizer.word_index\n",
        "test_seq = tokenizer.texts_to_sequences(test_data['Tweet_Final'].tolist())\n",
        "test_word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53Ceno8yL5im"
      },
      "source": [
        "###Applying padding\n",
        "Padding (0 values) applied to all tweets such that they are all 50 words long"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sKceJaCL3aG"
      },
      "source": [
        "#using keras.pad_sequences\n",
        "#each tweet is in the cnn data represented as a list of integers, each integer indexing a word.\n",
        "#padding adds 0's at the end of each list so they are all the same size, all of lenght 50\n",
        "max_lenght = 50 #sets the max lenght of a tweet, in number of words, 50 based on data analysis.\n",
        "cnn_training_data = pad_sequences(train_seq, \n",
        "                               maxlen=max_lenght) \n",
        "cnn_testing_data = pad_sequences(test_seq, \n",
        "                               maxlen=max_lenght)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk4t-8OAMT81"
      },
      "source": [
        "###Create word embedding \n",
        "Create the train_embeddings (word to vector) for each word in the vocabulary, using the word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0DzjtXFV_kE"
      },
      "source": [
        "train_embeddings = np.zeros((train_vocab_lenght+1, \n",
        " word2vec_dim))\n",
        "\n",
        "for word,ind in train_word_index.items():\n",
        "    train_embeddings[ind,:] = word2vec[word] if word in word2vec else np.random.rand(word2vec_dim) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flUEksH4O-3y"
      },
      "source": [
        "##Creating the CNN architecture\n",
        "Here the CNN is designed and initialized. We hare set decide on the layers, kernel sizes, how many nodes are to be used and the activation functions. This is discussed in section *4.2.2 Creating, and training, our own CNN using Keras*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr1vaRAjWlL4"
      },
      "source": [
        "def CNN(embeddings, max_length, vocab_lenght, embedding_dimensions):\n",
        "    embedding_layer = Embedding(vocab_lenght,\n",
        "                            embedding_dimensions,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_length,\n",
        "                            trainable=False) #use Keras.Embedding to create the embedding layer based on the weights from the word2vec model\n",
        "  \n",
        "    seq_input = Input(shape=(max_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(seq_input)\n",
        "\n",
        "    kernel_sizes = [1,2,3,4,5,6]\n",
        "    convolutions = []\n",
        "    for kernel_size in kernel_sizes:\n",
        "        conv1 = Conv1D(filters=325, \n",
        "                        kernel_size=kernel_size, \n",
        "                        activation='relu')(embedded_sequences)\n",
        "        max_pool = GlobalMaxPooling1D()(conv1)\n",
        "        convolutions.append(max_pool)\n",
        "    merge = concatenate(convolutions, axis=1)\n",
        "    var = Dropout(0.12)(merge) #certain units to be ignored, as to minimize overfitting \n",
        "    var = Dense(120, activation='relu')(var)\n",
        "    var = Dropout(0.18)(var) #certain units to be ignored as to minimize overfitting \n",
        "    sigmoid_pred = Dense(2, activation='sigmoid')(var) #sigmoid for prediction\n",
        "    cnn_model = Model(seq_input, sigmoid_pred)\n",
        "    cnn_model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy']) #use crossentroypy for loss function\n",
        "    cnn_model.summary()\n",
        "    return cnn_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYmpv3VCPCSf"
      },
      "source": [
        "initialize the CNN defined over"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WPOub-1XEmE"
      },
      "source": [
        "cnn_model = CNN(train_embeddings, \n",
        "                max_lenght, \n",
        "                train_vocab_lenght+1, \n",
        "                word2vec_dim) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm0BEeooPGAT"
      },
      "source": [
        "##Training the CNN on the training data\n",
        "The initialized CNN is trained on the training data. Here the validation_share is set to 10% (share of training data used for validation), epochs = 10 and batch size = 400. This is discussed in section *5.2 Convolutional neural network (CNN) results*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf-7eGWFXwFD"
      },
      "source": [
        "#training the CNN\n",
        "parties = ['dem', 'rep']\n",
        "y_train = training_data[parties].values\n",
        "x_train = cnn_training_data\n",
        "\n",
        "epochs = 10 #number of times data is passed through\n",
        "batch = 400#number of data points passed through at once\n",
        "validation_share = 0.1 #10% of training data is used as validation data\n",
        "history = cnn_model.fit(x_train, \n",
        "                 y_train, \n",
        "                 epochs=epochs, \n",
        "                 validation_split=validation_share, \n",
        "                 shuffle=True, \n",
        "                 batch_size=batch) #CNN model is trained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E93Z0b3bNCgj"
      },
      "source": [
        "Plotting training- and validation accuracy.\n",
        "Plotting training- and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ9AuKGerqdy"
      },
      "source": [
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# plot of accuracy history for training of CNN\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('accuracy of model')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# plot of loss history for training of CNN\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('loss of model')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BSF7xHFJuGD"
      },
      "source": [
        "##Testing the CNN on the test data\n",
        "Here we are applying the fully trained CNN on the test data, and the accuracy (and recall) is recorded. This is discussed in section *5.2 Convolutional neural network (CNN) results*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ_Zf3J7YMcn"
      },
      "source": [
        "#make prediction on test data using trained CNN\n",
        "predictions = cnn_model.predict(cnn_testing_data, \n",
        "                            batch_size=400, \n",
        "                            verbose=0)\n",
        "\n",
        "party_labels = [\"Democrat\", \"Republican\"]\n",
        "prediction_parties=[]\n",
        "\n",
        "for prediction in predictions:\n",
        "    prediction_parties.append(party_labels[np.argmax(prediction)])\n",
        "\n",
        "#present the results! \n",
        "print(\"Test dataset party split:\")\n",
        "print(test_data.Party.value_counts())\n",
        "print(\"test accuracy: \" + str(sum(test_data.Party==prediction_parties)/len(test_data)))\n",
        "i = 0\n",
        "demCorrect = 0\n",
        "repCorrect = 0\n",
        "for party in test_data.Party:\n",
        "  if party == \"Democrat\":\n",
        "    if prediction_parties[i] == \"Democrat\":\n",
        "      demCorrect += 1\n",
        "  elif party == \"Republican\":\n",
        "    if prediction_parties[i] == \"Republican\":\n",
        "      repCorrect += 1\n",
        "  i += 1\n",
        "print(\"Democrat tweets correctly guessed: \" + str(demCorrect))\n",
        "print(\"Republican tweets correctly guessed: \" + str(repCorrect))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}